	https://www.ibm.com/think/topics/knn
### What
	Non-parametric, supervised learning classifier, uses proximity, to classify or predict about the grouping of individual data point
### Mostly used for classification though can be used for regression
Class label is assigned based on majority vote (technically plurality voting since it can be more than 2 classes)
### Requirements:
1. Determine distance metrics
2. Defining k
#### Distance Measures
1. Euclidian distance (p=2)
	Most commonly used. Limited to real value vectors. Measures a straight line between query point and the other point being measured
	![[Pasted image 20250805192334.png]]
2. Manhattan distance (p=1) Other names: Taxicab, City Block
	Popular distance metrics. Measures the absolute value between two points. Commonly visualized with a grid 
	![[EuclideanDistance.png]]
3. Minkowski distance
	Generalized from Euclidian and Manhattan. p in the formula allows for the creation of other distance metrics. 
	p=2 -> Euclidian
	p=1 -> Manhattan
	![[MinkowskiDistance.png]]
4. Hamming distance. Other name: Overlap metric
	Typically used with Boolean or string vectors. identifying points where vectors don't match.
	![[HammingDistance.png]]
#### Defining k
	k value defines how many neighbors will be checked to determine the classification of the query point. 
	 
Different values can lead to overfitting and underfitting
- Lower values -> High variance, Low bias
- Higher values -> Low variance, High bias

##### The choice of k largely depends on the input data 
- Data with more outliers / noise -> Better with higher values of k

Optimal value for k can be achieved through:
- Odd number (avoid ties in classification)
- Cross validation tactics 
#### Applications of k-NN in machine learning 
- Data preprocessing: estimate missing values (missing data imputation)
- Recommendation engines: 
- Healthcare: Predict risk of heart attacks and prostate cancer. Calculating most likely gene expressions
- Pattern recognition: 

#### Advantages and disadvantages
- Advantages
	1. Easy to implement
	2. Adapts easily
	3. Few hyperparameters
- Disadvantages
	1. Doesnt scale well
	2. Curse of dimensionality
	3. Prone to overfitting
### Keywords: 

1. Parametric -> relating to or expressed in terms of a parameter or parameters.
2. Proximity -> nearness
3. Euclidian distance
4. Lazy learning